{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94ef78ea-2882-4841-8478-731cdd0ea3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP Introduction and Text Preprocessing Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d15b670e-2c80-4101-b1db-0beeaa666b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nagavardhan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nagavardhan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nagavardhan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nagavardhan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nagavardhan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nagavardhan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nagavardhan\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "import nltk\n",
    "nltk.data.path.append('C:\\\\Users\\\\nagavardhan\\\\nltk_data')  # Adjust the path if needed\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23efdd33-5a19-4196-af8c-f359dc1a0a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nagavardhan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'text', '.', 'It', 'includes', 'multiple', 'words', ',', 'such', 'as', 'email', '@', 'example.com', '.']\n",
      "['This is an example text.', 'It includes multiple words, such as email@example.com.']\n",
      "['example', 'text', '.', 'includes', 'multiple', 'words', ',', 'email', '@', 'example.com', '.']\n",
      "run\n",
      "running\n",
      "this is an example text it includes multiple words such as emailexamplecom\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk import FreqDist\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# 1 How can you perform word tokenization using NLTK\n",
    "def word_tokenization(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# 2 How can you perform sentence tokenization using NLTK\n",
    "def sentence_tokenization(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "# 3 How can you remove stopwords from a sentence\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    return [word for word in word_tokens if word.lower() not in stop_words]\n",
    "\n",
    "# 4 How can you perform stemming on a word\n",
    "def stemming(word):\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "# 5 How can you perform lemmatization on a word\n",
    "def lemmatization(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "# 6 How can you normalize a text by converting it to lowercase and removing punctuation\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    return ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "# 7 How can you create a co-occurrence matrix for words in a corpus\n",
    "def co_occurrence_matrix(corpus):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return (X.T * X)  # Co-occurrence matrix\n",
    "\n",
    "# 8 How can you apply a regular expression to extract all email addresses from a text\n",
    "def extract_emails(text):\n",
    "    return re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', text)\n",
    "\n",
    "# 9 How can you perform word embedding using Word2Vec\n",
    "def word2vec_embedding(text):\n",
    "    sentences = [word_tokenize(text)]\n",
    "    model = Word2Vec(sentences, min_count=1)\n",
    "    return model.wv\n",
    "\n",
    "# 10 How can you use Doc2Vec to embed documents\n",
    "def doc2vec_embedding(text):\n",
    "    sentences = [word_tokenize(text)]\n",
    "    model = Doc2Vec(sentences, vector_size=20, window=2, min_count=1, workers=4)\n",
    "    return model.infer_vector(word_tokenize(text))\n",
    "\n",
    "# 11 How can you perform part-of-speech tagging\n",
    "def pos_tagging(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return pos_tag(tokens)\n",
    "\n",
    "# 12 How can you find the similarity between two sentences using cosine similarity\n",
    "def cosine_similarity_between_sentences(sentence1, sentence2):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([sentence1, sentence2])\n",
    "    return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "\n",
    "# 13 How can you extract named entities from a sentence\n",
    "def named_entity_recognition(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    return ne_chunk(tagged)\n",
    "\n",
    "# 14 How can you split a large document into smaller chunks of text\n",
    "def split_document_into_chunks(text, chunk_size=1000):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# 15 How can you calculate the TF-IDF (Term Frequency - Inverse Document Frequency) for a set of documents\n",
    "def calculate_tfidf(documents):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    return tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# 16 How can you apply tokenization, stopword removal, and stemming in one go\n",
    "def tokenize_stopwords_stemming(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    stemmed_tokens = [stemming(word) for word in filtered_tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "# 17 How can you visualize the frequency distribution of words in a sentence?\n",
    "def visualize_word_frequency(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    freq_dist = FreqDist(tokens)\n",
    "    freq_dist.plot()\n",
    "\n",
    "# Example usage:\n",
    "text = \"This is an example text. It includes multiple words, such as email@example.com.\"\n",
    "sentence = \"This is a test sentence for NLP.\"\n",
    "print(word_tokenization(text))\n",
    "print(sentence_tokenization(text))\n",
    "print(remove_stopwords(text))\n",
    "print(stemming(\"running\"))\n",
    "print(lemmatization(\"running\"))\n",
    "print(normalize_text(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b24c1-d34c-4194-a7b8-309aeef71138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
